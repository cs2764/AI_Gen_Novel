#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""æ•…äº‹çº¿ç”Ÿæˆé”™è¯¯åˆ†æå·¥å…·"""

import json
import os
import glob
from datetime import datetime
from collections import defaultdict

def analyze_storyline_errors():
    """åˆ†ææ•…äº‹çº¿ç”Ÿæˆé”™è¯¯æ•°æ®"""
    
    print("=== æ•…äº‹çº¿ç”Ÿæˆé”™è¯¯åˆ†ææŠ¥å‘Š ===")
    
    # æ£€æŸ¥é”™è¯¯æ•°æ®ç›®å½•
    error_dir = "metadata/storyline_errors"
    success_dir = "metadata/storyline_success"
    
    if not os.path.exists(error_dir):
        print("âŒ é”™è¯¯æ•°æ®ç›®å½•ä¸å­˜åœ¨")
        return
    
    # è¯»å–æ‰€æœ‰é”™è¯¯æ–‡ä»¶
    error_files = glob.glob(os.path.join(error_dir, "*.json"))
    success_files = glob.glob(os.path.join(success_dir, "*.json")) if os.path.exists(success_dir) else []
    
    print(f"ğŸ“Š æ‰¾åˆ° {len(error_files)} ä¸ªé”™è¯¯è®°å½•ï¼Œ{len(success_files)} ä¸ªæˆåŠŸè®°å½•")
    
    if not error_files:
        print("âœ… æš‚æ— é”™è¯¯è®°å½•")
        return
    
    # åˆ†æé”™è¯¯æ•°æ®
    error_stats = defaultdict(int)
    provider_stats = defaultdict(int)
    error_details = []
    
    for error_file in error_files:
        try:
            with open(error_file, 'r', encoding='utf-8') as f:
                error_data = json.load(f)
            
            error_type = error_data.get('error_type', 'unknown')
            provider = error_data.get('provider', 'unknown')
            
            error_stats[error_type] += 1
            provider_stats[provider] += 1
            
            error_details.append({
                'file': os.path.basename(error_file),
                'timestamp': error_data.get('timestamp'),
                'error_type': error_type,
                'provider': provider,
                'attempt': error_data.get('attempt_number', 1),
                'response_length': error_data.get('response_length', 0),
                'has_json_markers': error_data.get('analysis', {}).get('has_json_markers', False),
                'has_braces': error_data.get('analysis', {}).get('has_braces', False),
                'has_chapters_key': error_data.get('analysis', {}).get('has_chapters_key', False),
                'candidates_found': error_data.get('repair_attempts', {}).get('json_candidates_found', 0),
                'preview': error_data.get('analysis', {}).get('response_preview', '')[:100]
            })
            
        except Exception as e:
            print(f"âš ï¸ è¯»å–é”™è¯¯æ–‡ä»¶å¤±è´¥ {error_file}: {e}")
    
    # æ‰“å°ç»Ÿè®¡æŠ¥å‘Š
    print(f"\nğŸ“ˆ é”™è¯¯ç±»å‹ç»Ÿè®¡:")
    for error_type, count in sorted(error_stats.items(), key=lambda x: x[1], reverse=True):
        print(f"  {error_type}: {count} æ¬¡")
    
    print(f"\nğŸ”§ æä¾›å•†ç»Ÿè®¡:")
    for provider, count in sorted(provider_stats.items(), key=lambda x: x[1], reverse=True):
        print(f"  {provider}: {count} æ¬¡é”™è¯¯")
    
    # åˆ†ææœ€å¸¸è§çš„é”™è¯¯
    print(f"\nğŸ” è¯¦ç»†é”™è¯¯åˆ†æ:")
    
    # æŒ‰é”™è¯¯ç±»å‹åˆ†ç»„
    errors_by_type = defaultdict(list)
    for error in error_details:
        errors_by_type[error['error_type']].append(error)
    
    for error_type, errors in errors_by_type.items():
        print(f"\n--- {error_type} ({len(errors)} æ¬¡) ---")
        
        # åˆ†æè¿™ç±»é”™è¯¯çš„ç‰¹å¾
        has_json_markers = sum(1 for e in errors if e['has_json_markers'])
        has_braces = sum(1 for e in errors if e['has_braces'])
        has_chapters = sum(1 for e in errors if e['has_chapters_key'])
        avg_length = sum(e['response_length'] for e in errors) / len(errors) if errors else 0
        avg_candidates = sum(e['candidates_found'] for e in errors) / len(errors) if errors else 0
        
        print(f"  ç‰¹å¾åˆ†æ:")
        print(f"    - åŒ…å«JSONæ ‡è®°: {has_json_markers}/{len(errors)} ({has_json_markers/len(errors)*100:.1f}%)")
        print(f"    - åŒ…å«å¤§æ‹¬å·: {has_braces}/{len(errors)} ({has_braces/len(errors)*100:.1f}%)")
        print(f"    - åŒ…å«chapterså…³é”®å­—: {has_chapters}/{len(errors)} ({has_chapters/len(errors)*100:.1f}%)")
        print(f"    - å¹³å‡å“åº”é•¿åº¦: {avg_length:.0f} å­—ç¬¦")
        print(f"    - å¹³å‡æ‰¾åˆ°å€™é€‰: {avg_candidates:.1f} ä¸ª")
        
        # æ˜¾ç¤ºæœ€è¿‘çš„å‡ ä¸ªé”™è¯¯ç¤ºä¾‹
        recent_errors = sorted(errors, key=lambda x: x['timestamp'], reverse=True)[:3]
        print(f"  æœ€è¿‘é”™è¯¯ç¤ºä¾‹:")
        for i, error in enumerate(recent_errors, 1):
            print(f"    {i}. {error['timestamp']} - å°è¯•{error['attempt']} - {error['response_length']}å­—ç¬¦")
            if error['preview']:
                print(f"       é¢„è§ˆ: {error['preview']}...")
    
    # åˆ†ææˆåŠŸæ¡ˆä¾‹
    if success_files:
        print(f"\nâœ… æˆåŠŸæ¡ˆä¾‹åˆ†æ:")
        success_stats = defaultdict(int)
        
        for success_file in success_files:
            try:
                with open(success_file, 'r', encoding='utf-8') as f:
                    success_data = json.load(f)
                
                method = success_data.get('method', 'unknown')
                success_stats[method] += 1
                
            except Exception as e:
                print(f"âš ï¸ è¯»å–æˆåŠŸæ–‡ä»¶å¤±è´¥ {success_file}: {e}")
        
        print(f"  æˆåŠŸæ–¹æ³•ç»Ÿè®¡:")
        for method, count in sorted(success_stats.items(), key=lambda x: x[1], reverse=True):
            print(f"    {method}: {count} æ¬¡æˆåŠŸ")
    
    # è¯»å–æ€»ä½“ç»Ÿè®¡
    stats_file = "metadata/storyline_error_stats.json"
    if os.path.exists(stats_file):
        try:
            with open(stats_file, 'r', encoding='utf-8') as f:
                total_stats = json.load(f)
            
            print(f"\nğŸ“Š æ€»ä½“ç»Ÿè®¡:")
            print(f"  æ€»é”™è¯¯æ•°: {total_stats.get('total_errors', 0)}")
            print(f"  æœ€åæ›´æ–°: {total_stats.get('last_updated', 'unknown')}")
            
        except Exception as e:
            print(f"âš ï¸ è¯»å–æ€»ä½“ç»Ÿè®¡å¤±è´¥: {e}")
    
    # æä¾›æ”¹è¿›å»ºè®®
    print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")
    
    if error_stats.get('json_parse_error', 0) > 0:
        print("  - JSONè§£æé”™è¯¯è¾ƒå¤šï¼Œè€ƒè™‘æ”¹è¿›æç¤ºè¯æ ¼å¼è¦æ±‚")
    
    if error_stats.get('json_repair_failed', 0) > 0:
        print("  - JSONä¿®å¤å¤±è´¥è¾ƒå¤šï¼Œè€ƒè™‘å¢å¼ºä¿®å¤ç®—æ³•")
    
    if error_stats.get('no_content_returned', 0) > 0:
        print("  - APIæ— è¿”å›å†…å®¹ï¼Œæ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAPIé…ç½®")
    
    # è®¡ç®—æˆåŠŸç‡
    total_attempts = len(error_files) + len(success_files)
    if total_attempts > 0:
        success_rate = len(success_files) / total_attempts * 100
        print(f"\nğŸ“ˆ å½“å‰æˆåŠŸç‡: {success_rate:.1f}% ({len(success_files)}/{total_attempts})")

def clean_old_error_data(days_to_keep=30):
    """æ¸…ç†æ—§çš„é”™è¯¯æ•°æ®"""
    print(f"\nğŸ§¹ æ¸…ç† {days_to_keep} å¤©å‰çš„é”™è¯¯æ•°æ®...")
    
    from datetime import timedelta
    cutoff_date = datetime.now() - timedelta(days=days_to_keep)
    
    for directory in ["metadata/storyline_errors", "metadata/storyline_success"]:
        if not os.path.exists(directory):
            continue
        
        files = glob.glob(os.path.join(directory, "*.json"))
        cleaned_count = 0
        
        for file_path in files:
            try:
                # ä»æ–‡ä»¶åæå–æ—¥æœŸ
                filename = os.path.basename(file_path)
                if "storyline_" in filename:
                    date_part = filename.split("_")[2]  # æ ¼å¼: storyline_error_20250715_...
                    file_date = datetime.strptime(date_part, "%Y%m%d")
                    
                    if file_date < cutoff_date:
                        os.remove(file_path)
                        cleaned_count += 1
                        
            except Exception as e:
                print(f"âš ï¸ æ¸…ç†æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        
        print(f"  {directory}: æ¸…ç†äº† {cleaned_count} ä¸ªæ–‡ä»¶")

if __name__ == "__main__":
    analyze_storyline_errors()
    
    # è¯¢é—®æ˜¯å¦æ¸…ç†æ—§æ•°æ®
    response = input("\næ˜¯å¦æ¸…ç†30å¤©å‰çš„æ—§é”™è¯¯æ•°æ®? (y/N): ")
    if response.lower() == 'y':
        clean_old_error_data()
