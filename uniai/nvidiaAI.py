import os
import time
import httpx
from openai import OpenAI


def nvidiaChatLLM(model_name="deepseek-ai/deepseek-v3.2", api_key=None, system_prompt="", base_url=None, thinking_enabled=False):
    """
    NVIDIA AI Chat LLM using OpenAI-compatible API
    
    Args:
        model_name: NVIDIA model name (default: deepseek-ai/deepseek-v3.2)
        api_key: NVIDIA API key
        system_prompt: System prompt to prepend to user messages
        base_url: API base URL (default: https://integrate.api.nvidia.com/v1)
        thinking_enabled: Enable thinking/reasoning mode (default: True)
    
    model_name å–å€¼ç¤ºä¾‹:
    - deepseek-ai/deepseek-v3.2
    - meta/llama-3.3-70b-instruct
    - qwen/qwen3-235b-instruct
    """
    api_key = os.environ.get("NVIDIA_API_KEY", api_key)
    
    # ä½¿ç”¨ä¼ å…¥çš„base_urlæˆ–é»˜è®¤å€¼
    actual_base_url = base_url or "https://integrate.api.nvidia.com/v1"
    
    # ä½¿ç”¨è¯¦ç»†çš„httpxè¶…æ—¶é…ç½®ï¼Œç¡®ä¿è¦†ç›–æ‰€æœ‰è¶…æ—¶åœºæ™¯
    # connect: è¿æ¥å»ºç«‹è¶…æ—¶ï¼ˆ30ç§’ï¼‰
    # read: è¯»å–æ•°æ®è¶…æ—¶ï¼ˆ30åˆ†é’Ÿï¼Œå› ä¸ºLLMç”Ÿæˆå¯èƒ½éœ€è¦å¾ˆé•¿æ—¶é—´ï¼‰
    # write: å†™å…¥æ•°æ®è¶…æ—¶ï¼ˆ60ç§’ï¼‰
    # pool: è¿æ¥æ± è·å–è¿æ¥è¶…æ—¶ï¼ˆ30ç§’ï¼‰
    custom_timeout = httpx.Timeout(
        connect=30.0,      # è¿æ¥è¶…æ—¶30ç§’
        read=1800.0,       # è¯»å–è¶…æ—¶30åˆ†é’Ÿï¼ˆ1800ç§’ï¼‰
        write=60.0,        # å†™å…¥è¶…æ—¶60ç§’
        pool=30.0          # è¿æ¥æ± è¶…æ—¶30ç§’
    )
    
    # ä½¿ç”¨NVIDIAçš„APIç«¯ç‚¹ - ä½¿ç”¨è¯¦ç»†çš„è¶…æ—¶é…ç½®
    client = OpenAI(
        api_key=api_key,
        base_url=actual_base_url,
        timeout=custom_timeout,  # ä½¿ç”¨è¯¦ç»†çš„httpxè¶…æ—¶é…ç½®
    )

    def chatLLM(
        messages: list,
        temperature=None,
        top_p=None,
        max_tokens=None,
        stream=False,  # NVIDIA APIé»˜è®¤ä½¿ç”¨éæµå¼æ¨¡å¼ä»¥é¿å…æµå¼è¾“å‡ºé—®é¢˜
    ) -> dict:

        
        # NVIDIA AIé»˜è®¤max_tokensè®¾ç½®ä¸º8192
        if max_tokens is None:
            max_tokens = 64000
        
        # å¦‚æœè®¾ç½®äº†ç³»ç»Ÿæç¤ºè¯ï¼Œåˆå¹¶åˆ°ç¬¬ä¸€ä¸ªç”¨æˆ·æ¶ˆæ¯çš„å¼€å¤´
        if system_prompt and messages:
            # æ‰¾åˆ°ç¬¬ä¸€ä¸ªç”¨æˆ·æ¶ˆæ¯
            for i, msg in enumerate(messages):
                if msg.get("role") == "user":
                    # å°†ç³»ç»Ÿæç¤ºè¯æ·»åŠ åˆ°ç”¨æˆ·æ¶ˆæ¯çš„å¼€å¤´
                    original_content = msg["content"]
                    messages[i]["content"] = f"{system_prompt}\n\n{original_content}"
                    break
            else:
                # å¦‚æœæ²¡æœ‰ç”¨æˆ·æ¶ˆæ¯ï¼Œåˆ›å»ºä¸€ä¸ªåŒ…å«ç³»ç»Ÿæç¤ºè¯çš„ç”¨æˆ·æ¶ˆæ¯
                messages.append({"role": "user", "content": system_prompt})
        
        # æ„å»ºè¯·æ±‚å‚æ•°
        params = {
            "model": model_name,
            "messages": messages,
        }
        
        # NVIDIA APIæ”¯æŒtemperatureå‚æ•°,èŒƒå›´é€šå¸¸ä¸º0-2
        if temperature is not None:
            try:
                # ç¡®ä¿temperatureæ˜¯æ•°å­—ç±»å‹
                temp_value = float(temperature)
                # ç¡®ä¿åœ¨åˆç†èŒƒå›´å†…,é¿å…APIé”™è¯¯
                validated_temp = max(0.0, min(2.0, temp_value))
                if validated_temp != temp_value:
                    print(f"âš ï¸ Temperature {temp_value} è¶…å‡ºèŒƒå›´,å·²è°ƒæ•´ä¸º {validated_temp}")
                params["temperature"] = validated_temp
                print(f"ğŸ”§ NVIDIA API: è®¾ç½® temperature = {validated_temp} (åŸå§‹å€¼: {temperature}, ç±»å‹: {type(temperature)})")
            except (TypeError, ValueError) as e:
                print(f"âŒ Temperature å‚æ•°æ— æ•ˆ: {temperature} (ç±»å‹: {type(temperature)}), é”™è¯¯: {e}")
                print(f"âš ï¸ è·³è¿‡ temperature å‚æ•°,ä½¿ç”¨APIé»˜è®¤å€¼")
        else:
            # é»˜è®¤ä½¿ç”¨temperature=1ï¼Œä¸NVIDIAç¤ºä¾‹ä¿æŒä¸€è‡´
            params["temperature"] = 1
        
        if top_p is not None:
            params["top_p"] = top_p
        else:
            # é»˜è®¤ä½¿ç”¨top_p=0.95ï¼Œä¸NVIDIAç¤ºä¾‹ä¿æŒä¸€è‡´
            params["top_p"] = 0.95
            
        if max_tokens is not None:
            params["max_tokens"] = max_tokens
        
        # å¯ç”¨æ€è€ƒæ¨¡å¼ (thinking_enabled=True æ—¶å¯ç”¨)
        # æ˜¾å¼è®¾ç½®æ€è€ƒæ¨¡å¼
        params["extra_body"] = {"chat_template_kwargs": {"thinking": thinking_enabled}}
        if thinking_enabled:
            print(f"ğŸ§  NVIDIA API: æ€è€ƒæ¨¡å¼å·²å¯ç”¨")
        
        try:
            if not stream:
                # è®°å½•APIè°ƒç”¨å¼€å§‹æ—¶é—´
                api_start_time = time.time()
                print(f"â±ï¸ NVIDIA API å¼€å§‹è°ƒç”¨ (éæµå¼)ï¼Œæ¨¡å‹: {model_name}")
                
                response = client.chat.completions.create(**params)
                
                # è®¡ç®—APIè°ƒç”¨è€—æ—¶
                api_elapsed = time.time() - api_start_time
                elapsed_minutes = api_elapsed / 60
                
                # è·å–å“åº”å†…å®¹
                
                content = ""
                reasoning_content = None
                
                if response.choices:
                    message = response.choices[0].message
                    content = message.content if message.content else ""
                    # å°è¯•è·å– reasoning_content (å¦‚æœå­˜åœ¨)
                    if hasattr(message, 'reasoning_content'):
                        reasoning_content = message.reasoning_content
                        
                    # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœcontentä¸ºç©ºä½†æœ‰reasoning_contentï¼Œä¸”çœ‹èµ·æ¥åƒæ­£æ–‡ï¼ˆä¸æ˜¯çº¯æ€è€ƒè¿‡ç¨‹ï¼‰
                    # æŸäº›NVIDIAæ¨¡å‹ä¼šå°†ç”Ÿæˆçš„æ­£æ–‡æ”¾åœ¨reasoningå­—æ®µä¸­
                    if not content and reasoning_content:
                        print(f"âš ï¸ [NVIDIA] Contentä¸ºç©ºï¼Œä½¿ç”¨reasoning_contentä½œä¸ºä¸»è¦å†…å®¹")
                        content = reasoning_content
                        # æ¸…ç©ºreasoning_contentä»¥é¿å…é‡å¤æ˜¾ç¤ºï¼ˆå¯é€‰ï¼Œå–å†³äºæ˜¯å¦æƒ³ä¿ç•™åŸå§‹ç»“æ„ï¼‰
                        # reasoning_content = None 
                    
                total_tokens = 0
                prompt_tokens = 0
                completion_tokens = 0
                
                if response.usage:
                    total_tokens = response.usage.total_tokens
                    prompt_tokens = response.usage.prompt_tokens
                    completion_tokens = response.usage.completion_tokens
                
                # è®°å½•APIè°ƒç”¨å®Œæˆæ—¥å¿—
                if elapsed_minutes > 1:
                    print(f"â±ï¸ NVIDIA API è°ƒç”¨å®Œæˆ: è€—æ—¶ {elapsed_minutes:.1f} åˆ†é’Ÿ, "
                          f"å“åº”é•¿åº¦ {len(content)} å­—ç¬¦, Tokenæ¶ˆè€— {total_tokens} (æé—®:{prompt_tokens}, å›å¤:{completion_tokens})")
                else:
                    print(f"â±ï¸ NVIDIA API è°ƒç”¨å®Œæˆ: è€—æ—¶ {api_elapsed:.1f} ç§’, "
                          f"å“åº”é•¿åº¦ {len(content)} å­—ç¬¦, Tokenæ¶ˆè€— {total_tokens} (æé—®:{prompt_tokens}, å›å¤:{completion_tokens})")
                
                # å¦‚æœè°ƒç”¨æ—¶é—´è¶…è¿‡10åˆ†é’Ÿï¼Œå‘å‡ºè­¦å‘Š
                if elapsed_minutes > 10:
                    print(f"âš ï¸âš ï¸ è­¦å‘Š: NVIDIA API è°ƒç”¨è€—æ—¶è¿‡é•¿ ({elapsed_minutes:.1f} åˆ†é’Ÿ)ï¼"
                          f"å¯èƒ½éœ€è¦æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–è€ƒè™‘åˆ†æ®µç”Ÿæˆã€‚")
                
                return {
                    "content": content,
                    "total_tokens": total_tokens,
                    "prompt_tokens": prompt_tokens,
                    "completion_tokens": completion_tokens,
                    "generation_time_ms": int(api_elapsed * 1000),  # è¿”å›ç”Ÿæˆæ—¶é—´ä¾›ä¸Šå±‚ç»Ÿè®¡
                    "reasoning_content": reasoning_content,
                }
            else:
                params["stream"] = True
                
                # è®°å½•æµå¼APIè°ƒç”¨å¼€å§‹æ—¶é—´
                stream_start_time = time.time()
                print(f"â±ï¸ NVIDIA API å¼€å§‹è°ƒç”¨ (æµå¼)ï¼Œæ¨¡å‹: {model_name}")
                
                responses = client.chat.completions.create(**params)

                def respGenerator():
                    content = ""
                    reasoning_content = ""
                    total_tokens = 0
                    last_progress_time = time.time()
                    last_content_length = 0
                    chunk_count = 0
                    
                    for response in responses:
                        chunk_count += 1
                        current_time = time.time()
                        
                        # è·³è¿‡æ²¡æœ‰choicesçš„response
                        if not getattr(response, "choices", None):
                            continue
                        
                        # å¤„ç†reasoning_content (æ€è€ƒè¿‡ç¨‹)
                        # ä»…åœ¨å¯ç”¨æ€è€ƒæ¨¡å¼æ—¶å¤„ç†
                        if thinking_enabled:
                            reasoning = getattr(response.choices[0].delta, "reasoning_content", None)
                            if reasoning:
                                reasoning_content += reasoning
                                # å®æ—¶yieldæ€è€ƒå†…å®¹ï¼ˆç”±aign_agents.pyè´Ÿè´£æ‰“å°åˆ°consoleï¼‰
                                yield {
                                    "content": content,
                                    "total_tokens": int(total_tokens),
                                    "reasoning_content": reasoning_content,
                                }
                        
                        # å¤„ç†å¸¸è§„content
                        if response.choices and response.choices[0].delta.content is not None:
                            delta = response.choices[0].delta.content
                            content += delta
                            
                            # ä¼°ç®—tokenæ•°é‡
                            total_tokens = len(content.split()) * 1.3
                            
                            # æ¯30ç§’æˆ–æ¯å¢åŠ 1000å­—ç¬¦æ—¶è¾“å‡ºè¿›åº¦æ—¥å¿—
                            elapsed_since_progress = current_time - last_progress_time
                            content_increase = len(content) - last_content_length
                            
                            if elapsed_since_progress >= 30 or content_increase >= 1000:
                                total_elapsed = current_time - stream_start_time
                                print(f"\nâ³ NVIDIA æµå¼ç”Ÿæˆè¿›åº¦: {len(content)} å­—ç¬¦, "
                                      f"{chunk_count} ä¸ªæ•°æ®å—, å·²è€—æ—¶ {total_elapsed:.1f} ç§’")
                                last_progress_time = current_time
                                last_content_length = len(content)
                            
                            yield {
                                "content": content,
                                "total_tokens": int(total_tokens),
                                "reasoning_content": reasoning_content,  # åŒ…å«æ€è€ƒè¿‡ç¨‹
                            }
                    
                    # æµå¼ç”Ÿæˆå®Œæˆæ—¥å¿—
                    total_elapsed = time.time() - stream_start_time
                    elapsed_minutes = total_elapsed / 60
                    if elapsed_minutes > 1:
                        print(f"\nâœ… NVIDIA æµå¼ç”Ÿæˆå®Œæˆ: æ€»è€—æ—¶ {elapsed_minutes:.1f} åˆ†é’Ÿ, "
                              f"æœ€ç»ˆé•¿åº¦ {len(content)} å­—ç¬¦, {chunk_count} ä¸ªæ•°æ®å—")
                    else:
                        print(f"\nâœ… NVIDIA æµå¼ç”Ÿæˆå®Œæˆ: æ€»è€—æ—¶ {total_elapsed:.1f} ç§’, "
                              f"æœ€ç»ˆé•¿åº¦ {len(content)} å­—ç¬¦, {chunk_count} ä¸ªæ•°æ®å—")
                    
                    if reasoning_content:
                        print(f"ğŸ§  æ€è€ƒè¿‡ç¨‹æ€»é•¿åº¦: {len(reasoning_content)} å­—ç¬¦")
                    
                    # é‡è¦ï¼šåœ¨æµç»“æŸåyieldæœ€ç»ˆçš„å®Œæ•´ç»“æœ
                    # è¿™ç¡®ä¿è°ƒç”¨æ–¹èƒ½è·å–åˆ°å®Œæ•´çš„å†…å®¹ï¼Œå³ä½¿æœ€åä¸€ä¸ªchunkæ²¡æœ‰åŒ…å«æ‰€æœ‰ä¿¡æ¯
                    yield {
                        "content": content,
                        "total_tokens": int(total_tokens),
                        "reasoning_content": reasoning_content,
                    }

                return respGenerator()
                
        except httpx.TimeoutException as e:
            # æ˜ç¡®å¤„ç†httpxè¶…æ—¶å¼‚å¸¸
            error_msg = str(e)
            print(f"âŒ NVIDIA API è¶…æ—¶é”™è¯¯: {error_msg}")
            if "read" in error_msg.lower():
                raise ValueError(f"NVIDIA APIè¯»å–è¶…æ—¶(30åˆ†é’Ÿ): æœåŠ¡å™¨å“åº”æ—¶é—´è¿‡é•¿ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–è€ƒè™‘å‡å°‘ç”Ÿæˆå†…å®¹é•¿åº¦ã€‚åŸå§‹é”™è¯¯: {error_msg}")
            elif "connect" in error_msg.lower():
                raise ValueError(f"NVIDIA APIè¿æ¥è¶…æ—¶(30ç§’): æ— æ³•è¿æ¥åˆ°APIæœåŠ¡å™¨ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥ã€‚åŸå§‹é”™è¯¯: {error_msg}")
            else:
                raise ValueError(f"NVIDIA APIè¶…æ—¶: {error_msg}")
        except httpx.HTTPStatusError as e:
            # å¤„ç†HTTPçŠ¶æ€ç é”™è¯¯
            print(f"âŒ NVIDIA API HTTPé”™è¯¯: {e.response.status_code} - {e.response.text[:200] if e.response.text else ''}")
            raise ValueError(f"NVIDIA API HTTPé”™è¯¯ {e.response.status_code}: {str(e)}")
        except Exception as e:
            error_msg = str(e)
            print(f"âŒ NVIDIA APIè°ƒç”¨å¤±è´¥: {error_msg}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯è¿æ¥ç›¸å…³é—®é¢˜
            connection_keywords = ['connection', 'timeout', 'reset', 'refused', 'network', 'unreachable']
            if any(keyword in error_msg.lower() for keyword in connection_keywords):
                print(f"ğŸ” æ£€æµ‹åˆ°å¯èƒ½çš„ç½‘ç»œé—®é¢˜: {error_msg}")
            
            raise ValueError(f"NVIDIA APIè°ƒç”¨å¤±è´¥: {error_msg}")

    return chatLLM
