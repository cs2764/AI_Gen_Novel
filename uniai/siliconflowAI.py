import os
from openai import OpenAI


def siliconflowChatLLM(model_name="deepseek-ai/DeepSeek-V3", api_key=None, system_prompt="", base_url=None, thinking_enabled=False):
    """
    SiliconFlow AI Chat LLM using OpenAI-compatible API
    
    Args:
        model_name: SiliconFlow model name (default: deepseek-ai/DeepSeek-V3)
        api_key: SiliconFlow API key
        system_prompt: System prompt to prepend to user messages
        base_url: API base URL (default: https://api.siliconflow.cn/v1)
    
    model_name å–å€¼ç¤ºä¾‹:
    - deepseek-ai/DeepSeek-V3
    - deepseek-ai/DeepSeek-R1
    - Qwen/Qwen2.5-72B-Instruct
    - Qwen/Qwen2.5-32B-Instruct
    - meta-llama/Llama-3.3-70B-Instruct
    - Pro/deepseek-ai/DeepSeek-V3
    - Pro/deepseek-ai/DeepSeek-V3
    - Pro/deepseek-ai/DeepSeek-R1
    
    Args:
        thinking_enabled: Enable thinking/reasoning mode (default: False)
    """
    api_key = os.environ.get("SILICONFLOW_API_KEY", api_key)
    
    # ä½¿ç”¨ä¼ å…¥çš„base_urlæˆ–é»˜è®¤å€¼
    actual_base_url = base_url or "https://api.siliconflow.cn/v1"
    
    # ä½¿ç”¨SiliconFlowçš„APIç«¯ç‚¹
    client = OpenAI(
        api_key=api_key,
        base_url=actual_base_url,
        timeout=1800.0,  # 30åˆ†é’Ÿè¶…æ—¶
    )

    def chatLLM(
        messages: list,
        temperature=None,
        top_p=None,
        max_tokens=None,
        stream=False,
    ) -> dict:

        
        # SiliconFlow AIé»˜è®¤max_tokensè®¾ç½®ä¸º32000ï¼ˆç¡®ä¿ç« èŠ‚å†…å®¹ä¸è¢«æˆªæ–­ï¼‰
        if max_tokens is None:
            max_tokens = 40000
        
        # å¦‚æœè®¾ç½®äº†ç³»ç»Ÿæç¤ºè¯ï¼Œåˆå¹¶åˆ°ç¬¬ä¸€ä¸ªç”¨æˆ·æ¶ˆæ¯çš„å¼€å¤´
        if system_prompt and messages:
            # æ‰¾åˆ°ç¬¬ä¸€ä¸ªç”¨æˆ·æ¶ˆæ¯
            for i, msg in enumerate(messages):
                if msg.get("role") == "user":
                    # å°†ç³»ç»Ÿæç¤ºè¯æ·»åŠ åˆ°ç”¨æˆ·æ¶ˆæ¯çš„å¼€å¤´
                    original_content = msg["content"]
                    messages[i]["content"] = f"{system_prompt}\n\n{original_content}"
                    break
            else:
                # å¦‚æœæ²¡æœ‰ç”¨æˆ·æ¶ˆæ¯ï¼Œåˆ›å»ºä¸€ä¸ªåŒ…å«ç³»ç»Ÿæç¤ºè¯çš„ç”¨æˆ·æ¶ˆæ¯
                messages.append({"role": "user", "content": system_prompt})
        
        # æ”¯æŒenable_thinkingå‚æ•°çš„æ¨¡å‹åˆ—è¡¨ï¼ˆæ ¹æ®SiliconFlowæ–‡æ¡£ï¼‰
        # https://docs.siliconflow.cn/cn/api-reference/chat-completions/chat-completions
        thinking_supported_models = [
            "zai-org/GLM-4.6",
            "Qwen/Qwen3-8B",
            "Qwen/Qwen3-14B",
            "Qwen/Qwen3-32B",
            "Qwen/Qwen3-30B-A3B",
            "Qwen/Qwen3-235B-A22B",
            "tencent/Hunyuan-A13B-Instruct",
            "zai-org/GLM-4.5V",
            "deepseek-ai/DeepSeek-V3.1-Terminus",
            "Pro/deepseek-ai/DeepSeek-V3.1-Terminus",
        ]
        
        # æ„å»ºè¯·æ±‚å‚æ•°
        params = {
            "model": model_name,
            "messages": messages,
        }
        
        # åªå¯¹æ”¯æŒçš„æ¨¡å‹æ·»åŠ enable_thinkingå‚æ•°ï¼Œä¸”ä»…åœ¨thinking_enabledä¸ºTrueæ—¶å¯ç”¨
        if thinking_enabled and any(model_name.startswith(m) or model_name == m for m in thinking_supported_models):
            params["enable_thinking"] = True
            print(f"ğŸ§  å·²ä¸ºæ¨¡å‹ {model_name} å¯ç”¨æ€è€ƒæ¨¡å¼ (enable_thinking=True)")
        elif thinking_enabled:
            # ç”¨æˆ·å¯ç”¨äº†æ€è€ƒæ¨¡å¼ä½†æ¨¡å‹ä¸æ”¯æŒï¼Œæ‰“å°è­¦å‘Š
             print(f"âš ï¸ æ¨¡å‹ {model_name} å¯èƒ½ä¸æ”¯æŒæ€è€ƒæ¨¡å¼ï¼Œä½†ç”¨æˆ·å·²å¯ç”¨ã€‚")
        
        # SiliconFlow APIæ”¯æŒtemperatureå‚æ•°,ä½†éœ€è¦ç¡®ä¿åœ¨æœ‰æ•ˆèŒƒå›´å†…
        # æ ¹æ®æ–‡æ¡£,é€šå¸¸èŒƒå›´æ˜¯0-2,ä½†æŸäº›æ¨¡å‹(å¦‚Claude)èŒƒå›´æ˜¯0-1
        if temperature is not None:
            try:
                # ç¡®ä¿temperatureæ˜¯æ•°å­—ç±»å‹
                temp_value = float(temperature)
                # ç¡®ä¿åœ¨åˆç†èŒƒå›´å†…,é¿å…APIé”™è¯¯
                validated_temp = max(0.0, min(2.0, temp_value))
                if validated_temp != temp_value:
                    print(f"âš ï¸ Temperature {temp_value} è¶…å‡ºèŒƒå›´,å·²è°ƒæ•´ä¸º {validated_temp}")
                params["temperature"] = validated_temp
                print(f"ğŸ”§ SiliconFlow API: è®¾ç½® temperature = {validated_temp} (åŸå§‹å€¼: {temperature}, ç±»å‹: {type(temperature)})")
            except (TypeError, ValueError) as e:
                print(f"âŒ Temperature å‚æ•°æ— æ•ˆ: {temperature} (ç±»å‹: {type(temperature)}), é”™è¯¯: {e}")
                print(f"âš ï¸ è·³è¿‡ temperature å‚æ•°,ä½¿ç”¨APIé»˜è®¤å€¼")
        if top_p is not None:
            params["top_p"] = top_p
        if max_tokens is not None:
            params["max_tokens"] = max_tokens  # ä¿ç•™åŸå§‹å‚æ•°
            params["max_completion_tokens"] = max_tokens  # æ·»åŠ APIå‚æ•°ï¼Œé™åˆ¶æ¨¡å‹ç”Ÿæˆå†…å®¹é•¿åº¦ï¼ˆåŒ…æ‹¬æ¨ç†è¿‡ç¨‹ï¼‰
        
        def _log_siliconflow_token_usage(usage):
            """æ˜¾ç¤ºSiliconFlow APIè¯¦ç»†Tokenä½¿ç”¨ä¿¡æ¯"""
            if not usage:
                return
            
            prompt_tokens = getattr(usage, 'prompt_tokens', 0) or 0
            completion_tokens = getattr(usage, 'completion_tokens', 0) or 0
            total_tokens = getattr(usage, 'total_tokens', 0) or 0
            
            # SiliconFlowç‰¹æœ‰çš„ç¼“å­˜ä¿¡æ¯
            prompt_cache_hit = getattr(usage, 'prompt_cache_hit_tokens', 0) or 0
            prompt_cache_miss = getattr(usage, 'prompt_cache_miss_tokens', 0) or 0
            
            # æ¨ç†Tokenè¯¦æƒ… (å¦‚æœæœ‰)
            reasoning_tokens = 0
            if hasattr(usage, 'completion_tokens_details') and usage.completion_tokens_details:
                reasoning_tokens = getattr(usage.completion_tokens_details, 'reasoning_tokens', 0) or 0
            
            # ç¼“å­˜Tokenè¯¦æƒ… (å¦‚æœæœ‰)
            cached_tokens = 0
            if hasattr(usage, 'prompt_tokens_details') and usage.prompt_tokens_details:
                cached_tokens = getattr(usage.prompt_tokens_details, 'cached_tokens', 0) or 0
            
            # è®¡ç®—ç¼“å­˜å‘½ä¸­ç‡
            cache_hit_rate = 0
            if prompt_tokens > 0:
                cache_hit_rate = (prompt_cache_hit / prompt_tokens) * 100
            
            # æ„å»ºæ˜¾ç¤ºä¿¡æ¯
            print("\n" + "="*60)
            print("ğŸ”¢ SiliconFlow Tokenä½¿ç”¨ç»Ÿè®¡:")
            print("-"*60)
            print(f"ğŸ“¥ è¾“å…¥Token: {prompt_tokens:,}")
            if prompt_cache_hit > 0 or prompt_cache_miss > 0:
                print(f"  â”œâ”€ ç¼“å­˜å‘½ä¸­: {prompt_cache_hit:,} ({cache_hit_rate:.1f}%)")
                print(f"  â””â”€ ç¼“å­˜æœªå‘½ä¸­: {prompt_cache_miss:,}")
            if cached_tokens > 0:
                print(f"  â””â”€ cached_tokens: {cached_tokens:,}")
            print(f"ğŸ“¤ è¾“å‡ºToken: {completion_tokens:,}")
            if reasoning_tokens > 0:
                print(f"  â””â”€ æ¨ç†Token: {reasoning_tokens:,}")
            print(f"ğŸ“Š æ€»Token: {total_tokens:,}")
            if prompt_cache_hit > 0:
                print(f"ğŸ’° èŠ‚çœToken: {prompt_cache_hit:,} (ç¼“å­˜å‘½ä¸­)")
            print("="*60 + "\n")

        def _extract_usage_dict(usage):
            """ä»usageå¯¹è±¡æå–è¯¦ç»†Tokenä¿¡æ¯å­—å…¸"""
            if not usage:
                return {}
            
            result = {
                "prompt_tokens": getattr(usage, 'prompt_tokens', 0) or 0,
                "completion_tokens": getattr(usage, 'completion_tokens', 0) or 0,
                "total_tokens": getattr(usage, 'total_tokens', 0) or 0,
                "prompt_cache_hit_tokens": getattr(usage, 'prompt_cache_hit_tokens', 0) or 0,
                "prompt_cache_miss_tokens": getattr(usage, 'prompt_cache_miss_tokens', 0) or 0,
            }
            
            # æ¨ç†Tokenè¯¦æƒ…
            if hasattr(usage, 'completion_tokens_details') and usage.completion_tokens_details:
                result["reasoning_tokens"] = getattr(usage.completion_tokens_details, 'reasoning_tokens', 0) or 0
            
            # ç¼“å­˜Tokenè¯¦æƒ…
            if hasattr(usage, 'prompt_tokens_details') and usage.prompt_tokens_details:
                result["cached_tokens"] = getattr(usage.prompt_tokens_details, 'cached_tokens', 0) or 0
            
            return result

        try:
            if not stream:
                response = client.chat.completions.create(**params)
                
                # æå–è¯¦ç»†Tokenä½¿ç”¨ä¿¡æ¯
                usage_dict = _extract_usage_dict(response.usage)
                
                # åœ¨æ§åˆ¶å°æ˜¾ç¤ºè¯¦ç»†Tokenç»Ÿè®¡
                _log_siliconflow_token_usage(response.usage)
                
                # è¿”å›åŒ…å«è¯¦ç»†Tokenä¿¡æ¯çš„å“åº”
                result = {
                    "content": response.choices[0].message.content,
                    "total_tokens": response.usage.total_tokens if response.usage else 0,
                }
                # æ·»åŠ è¯¦ç»†Tokenä¿¡æ¯
                result.update(usage_dict)
                return result
            else:
                params["stream"] = True
                # å¯ç”¨æµå¼è¿”å›ä¸­çš„usageä¿¡æ¯
                params["stream_options"] = {"include_usage": True}
                responses = client.chat.completions.create(**params)

                def respGenerator():
                    content = ""
                    reasoning_content = ""  # ç”¨äºç´¯ç§¯æ€è€ƒå†…å®¹
                    total_tokens = 0
                    final_usage = None
                    
                    for response in responses:
                        # æ£€æŸ¥æ˜¯å¦æœ‰usageä¿¡æ¯ï¼ˆæµå¼æ¨¡å¼æœ€åä¸€ä¸ªchunkä¼šåŒ…å«ï¼‰
                        if hasattr(response, 'usage') and response.usage:
                            final_usage = response.usage
                        
                        if response.choices and len(response.choices) > 0:
                            delta = response.choices[0].delta
                            
                            # å¤„ç†æ€è€ƒå†…å®¹ï¼ˆreasoning_contentï¼‰
                            if hasattr(delta, 'reasoning_content') and delta.reasoning_content:
                                new_reasoning = delta.reasoning_content
                                reasoning_content += new_reasoning
                                # å®æ—¶yieldæ€è€ƒå†…å®¹ï¼ˆç”±aign_agents.pyè´Ÿè´£æ‰“å°åˆ°consoleï¼‰
                                yield {
                                    "content": content,
                                    "reasoning_content": reasoning_content,
                                    "total_tokens": int(total_tokens),
                                }
                            
                            # å¤„ç†æ­£æ–‡å†…å®¹
                            if hasattr(delta, 'content') and delta.content:
                                new_content = delta.content
                                content += new_content
                                
                                # ä¼°ç®—tokenæ•°é‡ï¼ˆåœ¨æœ€ç»ˆusageè¿”å›å‰ä½¿ç”¨ä¼°ç®—å€¼ï¼‰
                                total_tokens = len(content.split()) * 1.3
                                
                                yield {
                                    "content": content,
                                    "reasoning_content": reasoning_content,
                                    "total_tokens": int(total_tokens),
                                }
                    
                    # æµç»“æŸåï¼Œå¦‚æœæœ‰usageä¿¡æ¯ï¼Œæ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡
                    if final_usage:
                        _log_siliconflow_token_usage(final_usage)
                        usage_dict = _extract_usage_dict(final_usage)
                        
                        # ç”Ÿæˆæœ€ç»ˆçš„åŒ…å«è¯¦ç»†Tokenä¿¡æ¯çš„ç»“æœ
                        final_result = {
                            "content": content,
                            "reasoning_content": reasoning_content,
                            "total_tokens": final_usage.total_tokens if final_usage else int(total_tokens),
                        }
                        final_result.update(usage_dict)
                        yield final_result

                return respGenerator()
                
        except Exception as e:
            raise ValueError(f"SiliconFlow APIè°ƒç”¨å¤±è´¥: {str(e)}")

    return chatLLM
