import os
import re

from openai import OpenAI


def _is_gpt_oss_model(model_name: str) -> bool:
    """æ£€æŸ¥æ˜¯å¦ä¸ºgpt-ossæ¨¡å‹"""
    return "gpt-oss" in model_name.lower()


def _build_harmony_prompt(messages: list, system_prompt: str = "") -> str:
    """
    ä¸ºgpt-ossæ¨¡å‹æ„å»ºHarmonyæ ¼å¼çš„æç¤ºè¯
    
    æ ¹æ®OpenAI Harmonyæ–‡æ¡£ï¼Œæ ¼å¼ä¸ºï¼š
    <|start|>role<|message|>content<|end|>
    
    æ”¯æŒçš„è§’è‰²: system, developer, user, assistant
    """
    parts = []
    
    print(f"ğŸ”§ æ„å»ºHarmonyæ ¼å¼æç¤ºè¯ï¼Œæ¶ˆæ¯æ•°é‡: {len(messages)}")
    
    # æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯ï¼ˆå¦‚æœæœ‰ï¼‰
    if system_prompt:
        # æ ¹æ®æ–‡æ¡£ï¼Œç³»ç»Ÿæ¶ˆæ¯å¯ä»¥åŒ…å«æ¨ç†ç­‰çº§è®¾ç½®
        system_content = system_prompt
        if "Reasoning:" not in system_content:
            system_content += "\n\nReasoning: high"
        parts.append(f"<|start|>system<|message|>{system_content}<|end|>")
        print(f"ğŸ”§ æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯ï¼Œé•¿åº¦: {len(system_content)} å­—ç¬¦")
    
    # å¤„ç†æ¶ˆæ¯åˆ—è¡¨
    for i, msg in enumerate(messages):
        role = msg.get("role", "user")
        content = msg.get("content", "")
        
        print(f"ğŸ”§ å¤„ç†æ¶ˆæ¯ {i+1}: [{role}] {len(content)} å­—ç¬¦")
        
        # æ˜ å°„è§’è‰²åˆ°Harmonyæ ¼å¼
        if role == "system" and not system_prompt:  # é¿å…é‡å¤ç³»ç»Ÿæ¶ˆæ¯
            parts.append(f"<|start|>system<|message|>{content}<|end|>")
        elif role == "assistant":
            # Assistantæ¶ˆæ¯éœ€è¦å®Œæ•´çš„ç»“æŸæ ‡è®°
            parts.append(f"<|start|>assistant<|message|>{content}<|end|>")
        elif role == "developer":
            # Developerè§’è‰²ç”¨äºæŒ‡ä»¤
            parts.append(f"<|start|>developer<|message|>{content}<|end|>")
        else:  # useræˆ–å…¶ä»–è§’è‰²
            parts.append(f"<|start|>user<|message|>{content}<|end|>")
    
    # æ·»åŠ åŠ©æ‰‹å¼€å§‹æ ‡è®°ï¼ˆä¸å®Œæ•´ï¼Œè®©æ¨¡å‹ç»§ç»­ï¼‰
    # æ ¹æ®æ–‡æ¡£ï¼Œè¿™é‡Œåº”è¯¥ä¸åŒ…å«<|message|>ï¼Œè®©æ¨¡å‹è‡ªå·±æ·»åŠ 
    parts.append("<|start|>assistant")
    
    final_prompt = "\n\n".join(parts)
    print(f"ğŸ”§ Harmonyæ ¼å¼æç¤ºè¯æ„å»ºå®Œæˆï¼Œæ€»é•¿åº¦: {len(final_prompt)} å­—ç¬¦")
    
    return final_prompt


def _parse_harmony_response(raw_response: str) -> str:
    """
    è§£æHarmonyæ ¼å¼çš„å“åº”ï¼Œæå–å®é™…å†…å®¹
    
    æ ¹æ®OpenAI Harmonyæ–‡æ¡£ï¼Œå“åº”æ ¼å¼å¯èƒ½åŒ…å«ï¼š
    - å¤šä¸ªchannel: analysis, commentary, final
    - ç‰¹æ®Šæ ‡è®°: <|channel|>, <|message|>, <|end|>
    """
    if not raw_response:
        return ""
    
    print(f"ğŸ”§ è§£æHarmonyæ ¼å¼å“åº”ï¼ŒåŸå§‹é•¿åº¦: {len(raw_response)} å­—ç¬¦")
    
    # 1. é¦–å…ˆå°è¯•æå–final channelçš„å†…å®¹ï¼ˆè¿™æ˜¯æœ€ç»ˆå›å¤ï¼‰
    final_patterns = [
        r'<\|channel\|>final<\|message\|>(.*?)(?:<\|end\||<\|channel\||$)',  # æ ‡å‡†final channel
        r'<\|channel\|>final\s*<\|message\|>(.*?)(?:<\|end\||<\|channel\||$)',  # å¸¦ç©ºæ ¼çš„å˜ä½“
    ]
    
    for pattern in final_patterns:
        final_match = re.search(pattern, raw_response, re.DOTALL)
        if final_match:
            final_content = final_match.group(1).strip()
            print(f"ğŸ”§ æå–åˆ°final channelå†…å®¹ï¼Œé•¿åº¦: {len(final_content)} å­—ç¬¦")
            return final_content
    
    # 2. å¦‚æœæ²¡æœ‰final channelï¼ŒæŸ¥æ‰¾æœ€åä¸€ä¸ªmessageæ ‡è®°çš„å†…å®¹
    message_patterns = [
        r'<\|message\|>(.*?)(?:<\|end\||<\|channel\||$)',  # æ ‡å‡†message
        r'<\|message\|>\s*(.*?)(?:<\|end\||<\|channel\||$)',  # å¸¦ç©ºæ ¼çš„message
    ]
    
    for pattern in message_patterns:
        message_matches = re.findall(pattern, raw_response, re.DOTALL)
        if message_matches:
            # å–æœ€åä¸€ä¸ªéç©ºæ¶ˆæ¯å†…å®¹
            for message in reversed(message_matches):
                message = message.strip()
                if message:
                    print(f"ğŸ”§ æå–åˆ°messageå†…å®¹ï¼Œé•¿åº¦: {len(message)} å­—ç¬¦")
                    return message
    
    # 3. å°è¯•æŸ¥æ‰¾assistantæ ‡è®°åçš„å†…å®¹ï¼ˆä¸å¸¦å…¶ä»–æ ‡è®°çš„ç®€å•æƒ…å†µï¼‰
    assistant_pattern = r'<\|start\|>assistant(?:<\|message\|>)?(.*?)(?:<\|end\||$)'
    assistant_match = re.search(assistant_pattern, raw_response, re.DOTALL)
    if assistant_match:
        assistant_content = assistant_match.group(1).strip()
        if assistant_content:
            print(f"ğŸ”§ æå–åˆ°assistantå†…å®¹ï¼Œé•¿åº¦: {len(assistant_content)} å­—ç¬¦")
            return assistant_content
    
    # 4. å¦‚æœä»¥ä¸Šéƒ½æ²¡æ‰¾åˆ°ï¼Œå°è¯•æ¸…ç†æ‰€æœ‰harmonyæ ‡è®°
    cleaned_response = raw_response
    # ç§»é™¤harmonyç‰¹æ®Šæ ‡è®°
    harmony_tags = [
        r'<\|start\|>[^<]*',
        r'<\|end\|>',
        r'<\|message\|>',
        r'<\|channel\|>[^<]*',
    ]
    
    for tag_pattern in harmony_tags:
        cleaned_response = re.sub(tag_pattern, '', cleaned_response)
    
    cleaned_response = cleaned_response.strip()
    
    if cleaned_response and cleaned_response != raw_response:
        print(f"ğŸ”§ æ¸…ç†Harmonyæ ‡è®°åçš„å†…å®¹ï¼Œé•¿åº¦: {len(cleaned_response)} å­—ç¬¦")
        return cleaned_response
    
    # 5. æœ€åçš„å¤‡é€‰æ–¹æ¡ˆï¼šè¿”å›åŸå§‹å“åº”
    print(f"âš ï¸ æ— æ³•è§£æHarmonyæ ¼å¼ï¼Œè¿”å›åŸå§‹å“åº”ï¼ˆé•¿åº¦: {len(raw_response)} å­—ç¬¦ï¼‰")
    return raw_response


def lmstudioChatLLM(model_name="local-model", base_url=None, api_key=None, system_prompt=""):
    """
    LM Studio API æ¥å£ï¼ˆä½¿ç”¨ Completions æ¨¡å¼ï¼Œè€Œé Chat Completionsï¼‰

    å‚æ•°è¯´æ˜:
    - model_name: æ¨¡å‹åç§°ï¼Œå¯ä»¥æ˜¯ä»»ä½•åœ¨LM Studioä¸­åŠ è½½çš„æ¨¡å‹
    - base_url: LM StudioæœåŠ¡å™¨åœ°å€ï¼Œé»˜è®¤ä¸º http://localhost:1234/v1
    - api_key: APIå¯†é’¥ï¼ŒLM Studioé€šå¸¸ä¸éœ€è¦ï¼Œå¯ä»¥ä¸ºä»»æ„å€¼
    - system_prompt: ç³»ç»Ÿæç¤ºè¯
    """
    base_url = base_url or os.environ.get("LM_STUDIO_BASE_URL", "http://localhost:1234/v1")
    api_key = api_key or os.environ.get("LM_STUDIO_API_KEY", "lm-studio")

    client = OpenAI(api_key=api_key, base_url=base_url, timeout=1800.0)  # 30åˆ†é’Ÿè¶…æ—¶

    def _build_completion_prompt(messages: list) -> str:
        parts = []
        if system_prompt:
            print(f"ğŸ”§ LM Studio æ¨¡å‹æä¾›å•†å±‚é¢ç³»ç»Ÿæç¤ºè¯é•¿åº¦: {len(system_prompt)} å­—ç¬¦")
            if len(system_prompt) > 100:
                print(f"ğŸ”§ ç³»ç»Ÿæç¤ºè¯å†…å®¹é¢„è§ˆ: {system_prompt[:200]}...")
                
            # æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤å†…å®¹
            if len(system_prompt) > 1000:
                lines = system_prompt.split('\n')
                print(f"ğŸ”§ ç³»ç»Ÿæç¤ºè¯è¡Œæ•°: {len(lines)}")
                if len(lines) > 10:
                    first_10_lines = '\n'.join(lines[:10])
                    print(f"ğŸ”§ å‰10è¡Œå†…å®¹: {first_10_lines[:300]}...")
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤çš„è¡Œ
                    line_counts = {}
                    for line in lines:
                        line_counts[line] = line_counts.get(line, 0) + 1
                    
                    repeated_lines = [(line, count) for line, count in line_counts.items() if count > 1 and len(line.strip()) > 10]
                    if repeated_lines:
                        print(f"ğŸ”§ å‘ç°é‡å¤è¡Œ: {len(repeated_lines)} ç§é‡å¤")
                        for line, count in repeated_lines[:3]:  # åªæ˜¾ç¤ºå‰3ç§
                            print(f"ğŸ”§   é‡å¤{count}æ¬¡: {line[:50]}...")
                    
            parts.append(f"System: {system_prompt}".strip())
        
        print(f"ğŸ”§ LM Studio å¤„ç†æ¶ˆæ¯åˆ—è¡¨ï¼Œå…± {len(messages)} æ¡æ¶ˆæ¯:")
        for i, msg in enumerate(messages or []):
            role = msg.get("role", "user")
            content = msg.get("content", "")
            content_len = len(content)
            print(f"ğŸ”§   æ¶ˆæ¯{i+1} [{role}]: {content_len} å­—ç¬¦")
            if content_len > 1000:
                print(f"ğŸ”§     å†…å®¹é¢„è§ˆ: {content[:200]}...")
            
            if role == "system":
                # å¦‚æœå·²ç»åœ¨æ¨¡å‹æä¾›å•†å±‚é¢è®¾ç½®äº†system_promptï¼Œè·³è¿‡æ¶ˆæ¯ä¸­çš„systemå†…å®¹
                # é¿å…é‡å¤æ·»åŠ ç³»ç»Ÿæç¤ºè¯
                if not system_prompt:
                    parts.append(f"System: {content}")
                else:
                    print(f"ğŸ”§     è·³è¿‡systemæ¶ˆæ¯ï¼ˆå·²æœ‰æ¨¡å‹æä¾›å•†å±‚é¢çš„system_promptï¼‰")
            elif role == "assistant":
                parts.append(f"Assistant: {content}")
            else:
                parts.append(f"User: {content}")
        
        # å¼•å¯¼æ¨¡å‹ç»§ç»­ä½œä¸ºåŠ©æ‰‹å›å¤
        if not parts or not parts[-1].startswith("Assistant:"):
            parts.append("Assistant:")
        
        final_prompt = "\n\n".join(parts)
        print(f"ğŸ”§ LM Studio æœ€ç»ˆæ„å»ºçš„æç¤ºè¯é•¿åº¦: {len(final_prompt)} å­—ç¬¦")
        
        # å¦‚æœæœ€ç»ˆæç¤ºè¯å¼‚å¸¸é•¿ï¼Œè¿›è¡Œé¢å¤–åˆ†æ
        if len(final_prompt) > 30000:
            print(f"âš ï¸  æœ€ç»ˆæç¤ºè¯å¼‚å¸¸é•¿ ({len(final_prompt)} å­—ç¬¦)ï¼Œè¿›è¡Œåˆ†æ:")
            parts_analysis = []
            for i, part in enumerate(parts):
                parts_analysis.append(f"  éƒ¨åˆ†{i+1}: {len(part)} å­—ç¬¦ - {part[:50]}...")
            print('\n'.join(parts_analysis[:5]))  # åªæ˜¾ç¤ºå‰5éƒ¨åˆ†
        
        return final_prompt

    def chatLLM(
        messages: list,
        temperature=None,
        top_p=None,
        max_tokens=None,
        stream=False,
        response_format=None,
        tools=None,
        tool_choice=None,
    ) -> dict:
        # Completions æ¨¡å¼ä¸æ”¯æŒå·¥å…·è°ƒç”¨/ç»“æ„åŒ–è¾“å‡ºï¼Œç»™å‡ºæç¤ºä½†ç»§ç»­æ­£å¸¸ç”Ÿæˆ
        if response_format is not None:
            print("âš ï¸ LM Studio Completions æ¨¡å¼ä¸æ”¯æŒ response_formatï¼Œå·²å¿½ç•¥")
        if tools is not None or tool_choice is not None:
            print("âš ï¸ LM Studio Completions æ¨¡å¼ä¸æ”¯æŒå·¥å…·è°ƒç”¨ï¼ˆtools/tool_choiceï¼‰ï¼Œå·²å¿½ç•¥")

        # æ£€æŸ¥æ˜¯å¦ä¸ºgpt-ossæ¨¡å‹ï¼Œä½¿ç”¨ç›¸åº”çš„æç¤ºè¯æ ¼å¼
        is_gpt_oss = _is_gpt_oss_model(model_name)
        if is_gpt_oss:
            print(f"ğŸ”§ æ£€æµ‹åˆ°gpt-ossæ¨¡å‹: {model_name}ï¼Œä½¿ç”¨Harmonyæ ¼å¼")
            prompt_text = _build_harmony_prompt(messages, system_prompt)
        else:
            print(f"ğŸ”§ ä½¿ç”¨æ ‡å‡†æ¨¡å‹: {model_name}ï¼Œä½¿ç”¨ä¼ ç»Ÿæ ¼å¼")
            prompt_text = _build_completion_prompt(messages)

        # æ„å»ºè¯·æ±‚å‚æ•°ï¼ˆCompletionsï¼‰
        params = {
            "model": model_name,
            "prompt": prompt_text,
        }

        # æŒ‰éœ€æ±‚ï¼šä¸åœ¨APIè°ƒç”¨ä¸­åŒ…å« temperature / top_p
        if max_tokens is not None:
            params["max_tokens"] = max_tokens
        else:
            # æ£€æµ‹æ˜¯å¦ä¸ºè¯¦ç»†å¤§çº²ç”Ÿæˆ
            is_detailed_outline = False
            try:
                # æ£€æŸ¥æ¶ˆæ¯å†…å®¹æ˜¯å¦åŒ…å«è¯¦ç»†å¤§çº²ç›¸å…³çš„å…³é”®è¯
                for msg in messages:
                    content = msg.get("content", "")
                    if any(keyword in content for keyword in ["è¯¦ç»†å¤§çº²", "DetailedOutlineGenerator", "å°è¯´è¯¦ç»†å¤§çº²æ‰©å±•ä¸“å®¶"]):
                        is_detailed_outline = True
                        break
            except Exception:
                pass
            
            # æ ¹æ®ä¸Šä¸‹æ–‡è®¾ç½®ä¸åŒçš„max_tokens
            if is_detailed_outline:
                params["max_tokens"] = 40000  # è¯¦ç»†å¤§çº²ç”Ÿæˆ
                print("ğŸ”§ LM Studio: æ£€æµ‹åˆ°è¯¦ç»†å¤§çº²ç”Ÿæˆï¼Œè®¾ç½®max_tokens=40000")
            else:
                params["max_tokens"] = 40000   # å…¶ä»–æƒ…å†µ
                print("ğŸ”§ LM Studio: å…¶ä»–æƒ…å†µï¼Œè®¾ç½®max_tokens=40000")

        try:
            if not stream:
                print("ğŸ”§ LM Studio: ä½¿ç”¨éæµå¼æ¨¡å¼")
                response = client.completions.create(**params)
                raw_text = response.choices[0].text if response and response.choices else ""
                
                # å¦‚æœæ˜¯gpt-ossæ¨¡å‹ï¼Œè§£æHarmonyæ ¼å¼
                if is_gpt_oss:
                    content = _parse_harmony_response(raw_text)
                    print(f"ğŸ”§ Harmonyæ ¼å¼è§£æå®Œæˆï¼Œæå–å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
                else:
                    content = raw_text
                
                return {
                    "content": content,
                    "total_tokens": getattr(getattr(response, "usage", None), "total_tokens", 0) or 0,
                }
            else:
                print("ğŸ”§ LM Studio: ä½¿ç”¨æµå¼æ¨¡å¼")
                stream_params = {**params, "stream": True}
                responses = client.completions.create(**stream_params)

                def respGenerator():
                    raw_content = ""
                    for response in responses:
                        delta_text = ""
                        try:
                            delta_text = response.choices[0].text
                        except Exception:
                            delta_text = ""
                        if delta_text:
                            raw_content += delta_text

                        # å¦‚æœæ˜¯gpt-ossæ¨¡å‹ï¼Œå°è¯•è§£æHarmonyæ ¼å¼
                        if is_gpt_oss:
                            # å¯¹äºæµå¼å“åº”ï¼Œæˆ‘ä»¬éœ€è¦ç­‰åˆ°æœ‰å®Œæ•´çš„æ ‡è®°æ‰èƒ½æ­£ç¡®è§£æ
                            # å…ˆè¿”å›åŸå§‹å†…å®¹ï¼Œæœ€åä¸€æ¬¡è¿­ä»£æ—¶è§£æå®Œæ•´æ ¼å¼
                            parsed_content = _parse_harmony_response(raw_content)
                            content = parsed_content if parsed_content != raw_content else raw_content
                        else:
                            content = raw_content

                        total_tokens = 0
                        if hasattr(response, "usage") and response.usage:
                            total_tokens = getattr(response.usage, "total_tokens", 0) or 0

                        yield {
                            "content": content,
                            "total_tokens": total_tokens,
                        }

                return respGenerator()

        except Exception as e:
            print(f"âŒ LM Studio API è°ƒç”¨å¤±è´¥: {e}")
            print(f"è¯·ç¡®ä¿ LM Studio æ­£åœ¨è¿è¡Œå¹¶ç›‘å¬ {base_url}")
            raise e

    return chatLLM