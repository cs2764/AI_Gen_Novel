import os
import re

from openai import OpenAI


def _is_gpt_oss_model(model_name: str) -> bool:
    """æ£€æŸ¥æ˜¯å¦ä¸ºgpt-ossæ¨¡å‹"""
    return "gpt-oss" in model_name.lower()


def _build_harmony_prompt(messages: list, system_prompt: str = "") -> str:
    """
    ä¸ºgpt-ossæ¨¡å‹æ„å»ºHarmonyæ ¼å¼çš„æç¤ºè¯
    
    æ ¹æ®OpenAI Harmonyæ–‡æ¡£ï¼Œæ ¼å¼ä¸ºï¼š
    <|start|>role<|message|>content<|end|>
    
    æ”¯æŒçš„è§’è‰²: system, developer, user, assistant
    """
    parts = []
    
    print(f"ğŸ”§ æ„å»ºHarmonyæ ¼å¼æç¤ºè¯ï¼Œæ¶ˆæ¯æ•°é‡: {len(messages)}")
    
    # æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯ï¼ˆå¦‚æœæœ‰ï¼‰
    if system_prompt:
        # æ ¹æ®æ–‡æ¡£ï¼Œç³»ç»Ÿæ¶ˆæ¯å¯ä»¥åŒ…å«æ¨ç†ç­‰çº§è®¾ç½®
        system_content = system_prompt
        if "Reasoning:" not in system_content:
            system_content += "\n\nReasoning: high"
        parts.append(f"<|start|>system<|message|>{system_content}<|end|>")
        print(f"ğŸ”§ æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯ï¼Œé•¿åº¦: {len(system_content)} å­—ç¬¦")
    
    # å¤„ç†æ¶ˆæ¯åˆ—è¡¨
    for i, msg in enumerate(messages):
        role = msg.get("role", "user")
        content = msg.get("content", "")
        
        print(f"ğŸ”§ å¤„ç†æ¶ˆæ¯ {i+1}: [{role}] {len(content)} å­—ç¬¦")
        
        # æ˜ å°„è§’è‰²åˆ°Harmonyæ ¼å¼
        if role == "system" and not system_prompt:  # é¿å…é‡å¤ç³»ç»Ÿæ¶ˆæ¯
            parts.append(f"<|start|>system<|message|>{content}<|end|>")
        elif role == "assistant":
            # Assistantæ¶ˆæ¯éœ€è¦å®Œæ•´çš„ç»“æŸæ ‡è®°
            parts.append(f"<|start|>assistant<|message|>{content}<|end|>")
        elif role == "developer":
            # Developerè§’è‰²ç”¨äºæŒ‡ä»¤
            parts.append(f"<|start|>developer<|message|>{content}<|end|>")
        else:  # useræˆ–å…¶ä»–è§’è‰²
            parts.append(f"<|start|>user<|message|>{content}<|end|>")
    
    # æ·»åŠ åŠ©æ‰‹å¼€å§‹æ ‡è®°ï¼ˆä¸å®Œæ•´ï¼Œè®©æ¨¡å‹ç»§ç»­ï¼‰
    # æ ¹æ®æ–‡æ¡£ï¼Œè¿™é‡Œåº”è¯¥ä¸åŒ…å«<|message|>ï¼Œè®©æ¨¡å‹è‡ªå·±æ·»åŠ 
    parts.append("<|start|>assistant")
    
    final_prompt = "\n\n".join(parts)
    print(f"ğŸ”§ Harmonyæ ¼å¼æç¤ºè¯æ„å»ºå®Œæˆï¼Œæ€»é•¿åº¦: {len(final_prompt)} å­—ç¬¦")
    
    return final_prompt


def _parse_harmony_response(raw_response: str) -> str:
    """
    è§£æHarmonyæ ¼å¼çš„å“åº”ï¼Œæå–å®é™…å†…å®¹
    
    æ ¹æ®OpenAI Harmonyæ–‡æ¡£ï¼Œå“åº”æ ¼å¼å¯èƒ½åŒ…å«ï¼š
    - å¤šä¸ªchannel: analysis, commentary, final
    - ç‰¹æ®Šæ ‡è®°: <|channel|>, <|message|>, <|end|>
    """
    if not raw_response:
        return ""
    
    print(f"ğŸ”§ è§£æHarmonyæ ¼å¼å“åº”ï¼ŒåŸå§‹é•¿åº¦: {len(raw_response)} å­—ç¬¦")
    
    # 1. é¦–å…ˆå°è¯•æå–final channelçš„å†…å®¹ï¼ˆè¿™æ˜¯æœ€ç»ˆå›å¤ï¼‰
    final_patterns = [
        r'<\|channel\|>final<\|message\|>(.*?)(?:<\|end\||<\|channel\||$)',  # æ ‡å‡†final channel
        r'<\|channel\|>final\s*<\|message\|>(.*?)(?:<\|end\||<\|channel\||$)',  # å¸¦ç©ºæ ¼çš„å˜ä½“
    ]
    
    for pattern in final_patterns:
        final_match = re.search(pattern, raw_response, re.DOTALL)
        if final_match:
            final_content = final_match.group(1).strip()
            print(f"ğŸ”§ æå–åˆ°final channelå†…å®¹ï¼Œé•¿åº¦: {len(final_content)} å­—ç¬¦")
            return final_content
    
    # 2. å¦‚æœæ²¡æœ‰final channelï¼ŒæŸ¥æ‰¾æœ€åä¸€ä¸ªmessageæ ‡è®°çš„å†…å®¹
    message_patterns = [
        r'<\|message\|>(.*?)(?:<\|end\||<\|channel\||$)',  # æ ‡å‡†message
        r'<\|message\|>\s*(.*?)(?:<\|end\||<\|channel\||$)',  # å¸¦ç©ºæ ¼çš„message
    ]
    
    for pattern in message_patterns:
        message_matches = re.findall(pattern, raw_response, re.DOTALL)
        if message_matches:
            # å–æœ€åä¸€ä¸ªéç©ºæ¶ˆæ¯å†…å®¹
            for message in reversed(message_matches):
                message = message.strip()
                if message:
                    print(f"ğŸ”§ æå–åˆ°messageå†…å®¹ï¼Œé•¿åº¦: {len(message)} å­—ç¬¦")
                    return message
    
    # 3. å°è¯•æŸ¥æ‰¾assistantæ ‡è®°åçš„å†…å®¹ï¼ˆä¸å¸¦å…¶ä»–æ ‡è®°çš„ç®€å•æƒ…å†µï¼‰
    assistant_pattern = r'<\|start\|>assistant(?:<\|message\|>)?(.*?)(?:<\|end\||$)'
    assistant_match = re.search(assistant_pattern, raw_response, re.DOTALL)
    if assistant_match:
        assistant_content = assistant_match.group(1).strip()
        if assistant_content:
            print(f"ğŸ”§ æå–åˆ°assistantå†…å®¹ï¼Œé•¿åº¦: {len(assistant_content)} å­—ç¬¦")
            return assistant_content
    
    # 4. å¦‚æœä»¥ä¸Šéƒ½æ²¡æ‰¾åˆ°ï¼Œå°è¯•æ¸…ç†æ‰€æœ‰harmonyæ ‡è®°
    cleaned_response = raw_response
    # ç§»é™¤harmonyç‰¹æ®Šæ ‡è®°
    harmony_tags = [
        r'<\|start\|>[^<]*',
        r'<\|end\|>',
        r'<\|message\|>',
        r'<\|channel\|>[^<]*',
    ]
    
    for tag_pattern in harmony_tags:
        cleaned_response = re.sub(tag_pattern, '', cleaned_response)
    
    cleaned_response = cleaned_response.strip()
    
    if cleaned_response and cleaned_response != raw_response:
        print(f"ğŸ”§ æ¸…ç†Harmonyæ ‡è®°åçš„å†…å®¹ï¼Œé•¿åº¦: {len(cleaned_response)} å­—ç¬¦")
        return cleaned_response
    
    # 5. æœ€åçš„å¤‡é€‰æ–¹æ¡ˆï¼šè¿”å›åŸå§‹å“åº”
    print(f"âš ï¸ æ— æ³•è§£æHarmonyæ ¼å¼ï¼Œè¿”å›åŸå§‹å“åº”ï¼ˆé•¿åº¦: {len(raw_response)} å­—ç¬¦ï¼‰")
    return raw_response


def lmstudioChatLLM(model_name="local-model", base_url=None, api_key=None, system_prompt=""):
    """
    LM Studio API æ¥å£ï¼ˆæ ‡å‡†æ¨¡å‹ä½¿ç”¨ Chat Completionsï¼Œgpt-ossæ¨¡å‹ä½¿ç”¨ Completions + Harmony æ ¼å¼ï¼‰

    å‚æ•°è¯´æ˜:
    - model_name: æ¨¡å‹åç§°ï¼Œå¯ä»¥æ˜¯ä»»ä½•åœ¨LM Studioä¸­åŠ è½½çš„æ¨¡å‹
    - base_url: LM StudioæœåŠ¡å™¨åœ°å€ï¼Œé»˜è®¤ä¸º http://localhost:1234/v1
    - api_key: APIå¯†é’¥ï¼ŒLM Studioé€šå¸¸ä¸éœ€è¦ï¼Œå¯ä»¥ä¸ºä»»æ„å€¼
    - system_prompt: ç³»ç»Ÿæç¤ºè¯
    """
    base_url = base_url or os.environ.get("LM_STUDIO_BASE_URL", "http://localhost:1234/v1")
    api_key = api_key or os.environ.get("LM_STUDIO_API_KEY", "lm-studio")

    client = OpenAI(api_key=api_key, base_url=base_url, timeout=1800.0)  # 30åˆ†é’Ÿè¶…æ—¶

    def _build_chat_messages(messages: list) -> list:
        """æ„å»ºæ ‡å‡† Chat Completions çš„ messages æ•°ç»„"""
        chat_messages = []
        
        # æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯
        if system_prompt:
            chat_messages.append({"role": "system", "content": system_prompt})
        
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        for msg in messages or []:
            role = msg.get("role", "user")
            content = msg.get("content", "")
            # è·³è¿‡é‡å¤çš„ç³»ç»Ÿæ¶ˆæ¯
            if role == "system" and system_prompt:
                continue
            chat_messages.append({"role": role, "content": content})
        
        return chat_messages

    def chatLLM(
        messages: list,
        temperature=None,
        top_p=None,
        max_tokens=None,
        stream=False,
        response_format=None,
        tools=None,
        tool_choice=None,
    ) -> dict:
        # æ£€æŸ¥æ˜¯å¦ä¸ºgpt-ossæ¨¡å‹ï¼ˆéœ€è¦ç‰¹æ®Šçš„ Harmony æ ¼å¼ï¼Œèµ° Completions ç«¯ç‚¹ï¼‰
        is_gpt_oss = _is_gpt_oss_model(model_name)
        
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ toolsï¼ˆFunction Callingï¼‰
        use_tools = tools is not None
        
        if use_tools:
            # ========== Chat Completionsæ¨¡å¼ï¼ˆæ”¯æŒFunction Callingï¼‰==========
            print(f"ğŸ”§ LM Studio: æ£€æµ‹åˆ°toolså‚æ•°ï¼Œä½¿ç”¨Chat Completionsæ¨¡å¼")
            print(f"ğŸ”§ LM Studio: toolsæ•°é‡={len(tools)}, tool_choice={tool_choice}")
            
            chat_messages = _build_chat_messages(messages)
            print(f"ğŸ”§ LM Studio Chat: æ¶ˆæ¯æ•°é‡={len(chat_messages)}")
            
            # æ„å»ºè¯·æ±‚å‚æ•°
            params = {
                "model": model_name,
                "messages": chat_messages,
                "tools": tools,
            }
            
            # è®¾ç½®tool_choiceï¼ˆLM Studioæ”¯æŒ "auto", "none", æˆ–å¯¹è±¡æ ¼å¼ï¼‰
            if tool_choice:
                params["tool_choice"] = tool_choice
            
            # è®¾ç½®max_tokens
            if max_tokens is not None:
                params["max_tokens"] = max_tokens
            else:
                params["max_tokens"] = 40000
            
            try:
                print("ğŸ”§ LM Studio: è°ƒç”¨chat.completions.create()ï¼ˆå¸¦toolsï¼‰...")
                response = client.chat.completions.create(**params)
                
                # å¤„ç†å“åº”
                if response and response.choices:
                    choice = response.choices[0]
                    message = choice.message
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
                    if hasattr(message, 'tool_calls') and message.tool_calls:
                        print(f"âœ… LM Studio: æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨ï¼Œæ•°é‡={len(message.tool_calls)}")
                        return {
                            "content": message.content or "",
                            "tool_calls": message.tool_calls,
                            "total_tokens": getattr(getattr(response, "usage", None), "total_tokens", 0) or 0,
                        }
                    else:
                        # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œè¿”å›æ™®é€šå†…å®¹
                        content = message.content or ""
                        print(f"âš ï¸ LM Studio: æœªæ£€æµ‹åˆ°å·¥å…·è°ƒç”¨ï¼Œè¿”å›æ™®é€šå†…å®¹ï¼ˆ{len(content)}å­—ç¬¦ï¼‰")
                        return {
                            "content": content,
                            "total_tokens": getattr(getattr(response, "usage", None), "total_tokens", 0) or 0,
                        }
                else:
                    print("âš ï¸ LM Studio: Chat Completionså“åº”ä¸ºç©º")
                    return {"content": "", "total_tokens": 0}
                    
            except Exception as e:
                print(f"âŒ LM Studio Chat Completionsè°ƒç”¨å¤±è´¥: {e}")
                print(f"è¯·ç¡®ä¿æ¨¡å‹æ”¯æŒfunction callingï¼ˆå¦‚Qwen2.5, Llama 3.1+, Mistralç­‰ï¼‰")
                raise e
        
        elif is_gpt_oss:
            # ========== gpt-ossæ¨¡å‹ï¼šä½¿ç”¨ Completions + Harmony æ ¼å¼ ==========
            if response_format is not None:
                print("âš ï¸ LM Studio Completions æ¨¡å¼ä¸æ”¯æŒ response_formatï¼Œå·²å¿½ç•¥")

            print(f"ğŸ”§ æ£€æµ‹åˆ°gpt-ossæ¨¡å‹: {model_name}ï¼Œä½¿ç”¨Harmonyæ ¼å¼")
            prompt_text = _build_harmony_prompt(messages, system_prompt)

            # æ„å»ºè¯·æ±‚å‚æ•°ï¼ˆCompletionsï¼‰
            params = {
                "model": model_name,
                "prompt": prompt_text,
            }

            if max_tokens is not None:
                params["max_tokens"] = max_tokens
            else:
                params["max_tokens"] = 40000

            try:
                if not stream:
                    print("ğŸ”§ LM Studio: ä½¿ç”¨éæµå¼æ¨¡å¼ï¼ˆHarmonyæ ¼å¼ï¼‰")
                    response = client.completions.create(**params)
                    raw_text = response.choices[0].text if response and response.choices else ""
                    
                    content = _parse_harmony_response(raw_text)
                    print(f"ğŸ”§ Harmonyæ ¼å¼è§£æå®Œæˆï¼Œæå–å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
                    
                    return {
                        "content": content,
                        "total_tokens": getattr(getattr(response, "usage", None), "total_tokens", 0) or 0,
                    }
                else:
                    print("ğŸ”§ LM Studio: ä½¿ç”¨æµå¼æ¨¡å¼ï¼ˆHarmonyæ ¼å¼ï¼‰")
                    stream_params = {**params, "stream": True}
                    responses = client.completions.create(**stream_params)

                    def respGenerator():
                        raw_content = ""
                        for response in responses:
                            delta_text = ""
                            try:
                                delta_text = response.choices[0].text
                            except Exception:
                                delta_text = ""
                            if delta_text:
                                raw_content += delta_text

                            parsed_content = _parse_harmony_response(raw_content)
                            content = parsed_content if parsed_content != raw_content else raw_content

                            total_tokens = 0
                            if hasattr(response, "usage") and response.usage:
                                total_tokens = getattr(response.usage, "total_tokens", 0) or 0

                            yield {
                                "content": content,
                                "total_tokens": total_tokens,
                            }

                    return respGenerator()

            except Exception as e:
                print(f"âŒ LM Studio API è°ƒç”¨å¤±è´¥ï¼ˆHarmonyæ ¼å¼ï¼‰: {e}")
                print(f"è¯·ç¡®ä¿ LM Studio æ­£åœ¨è¿è¡Œå¹¶ç›‘å¬ {base_url}")
                raise e
        
        else:
            # ========== æ ‡å‡†æ¨¡å‹ï¼šä½¿ç”¨ Chat Completions ==========
            print(f"ğŸ”§ ä½¿ç”¨æ ‡å‡†æ¨¡å‹: {model_name}ï¼Œä½¿ç”¨Chat Completionsæ¨¡å¼")
            
            chat_messages = _build_chat_messages(messages)
            print(f"ğŸ”§ LM Studio Chat: æ¶ˆæ¯æ•°é‡={len(chat_messages)}")
            
            # æ„å»ºè¯·æ±‚å‚æ•°
            params = {
                "model": model_name,
                "messages": chat_messages,
            }

            if max_tokens is not None:
                params["max_tokens"] = max_tokens
            else:
                params["max_tokens"] = 40000
                print("ğŸ”§ LM Studio: è®¾ç½®max_tokens=40000")

            try:
                if not stream:
                    print("ğŸ”§ LM Studio: ä½¿ç”¨éæµå¼Chat Completionsæ¨¡å¼")
                    response = client.chat.completions.create(**params)
                    
                    content = ""
                    if response and response.choices:
                        content = response.choices[0].message.content or ""
                    
                    print(f"âœ… LM Studio: è¿”å›å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
                    return {
                        "content": content,
                        "total_tokens": getattr(getattr(response, "usage", None), "total_tokens", 0) or 0,
                    }
                else:
                    print("ğŸ”§ LM Studio: ä½¿ç”¨æµå¼Chat Completionsæ¨¡å¼")
                    stream_params = {**params, "stream": True}
                    responses = client.chat.completions.create(**stream_params)

                    def respGenerator():
                        full_content = ""
                        for chunk in responses:
                            delta_content = ""
                            try:
                                delta = chunk.choices[0].delta
                                delta_content = delta.content or ""
                            except Exception:
                                delta_content = ""
                            if delta_content:
                                full_content += delta_content

                            total_tokens = 0
                            if hasattr(chunk, "usage") and chunk.usage:
                                total_tokens = getattr(chunk.usage, "total_tokens", 0) or 0

                            yield {
                                "content": full_content,
                                "total_tokens": total_tokens,
                            }

                    return respGenerator()

            except Exception as e:
                print(f"âŒ LM Studio Chat Completions è°ƒç”¨å¤±è´¥: {e}")
                print(f"è¯·ç¡®ä¿ LM Studio æ­£åœ¨è¿è¡Œå¹¶ç›‘å¬ {base_url}")
                raise e

    return chatLLM