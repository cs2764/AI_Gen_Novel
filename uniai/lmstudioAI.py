import os

from openai import OpenAI


def lmstudioChatLLM(model_name="local-model", base_url=None, api_key=None, system_prompt=""):
    """
    LM Studio API æ¥å£
    
    å‚æ•°è¯´æ˜:
    - model_name: æ¨¡å‹åç§°ï¼Œå¯ä»¥æ˜¯ä»»ä½•åœ¨LM Studioä¸­åŠ è½½çš„æ¨¡å‹
    - base_url: LM StudioæœåŠ¡å™¨åœ°å€ï¼Œé»˜è®¤ä¸º http://localhost:1234/v1
    - api_key: APIå¯†é’¥ï¼ŒLM Studioé€šå¸¸ä¸éœ€è¦ï¼Œå¯ä»¥ä¸ºä»»æ„å€¼
    - system_prompt: ç³»ç»Ÿæç¤ºè¯
    """
    base_url = base_url or os.environ.get("LM_STUDIO_BASE_URL", "http://localhost:1234/v1")
    api_key = api_key or os.environ.get("LM_STUDIO_API_KEY", "lm-studio")
    
    client = OpenAI(api_key=api_key, base_url=base_url)

    def chatLLM(
        messages: list,
        temperature=None,
        top_p=None,
        max_tokens=None,
        stream=False,
        response_format=None,
        tools=None,
        tool_choice=None,
    ) -> dict:
        # å¦‚æœè®¾ç½®äº†ç³»ç»Ÿæç¤ºè¯ï¼Œåˆå¹¶åˆ°ç¬¬ä¸€ä¸ªç”¨æˆ·æ¶ˆæ¯çš„å¼€å¤´
        if system_prompt and messages:
            # æ‰¾åˆ°ç¬¬ä¸€ä¸ªç”¨æˆ·æ¶ˆæ¯
            for i, msg in enumerate(messages):
                if msg.get("role") == "user":
                    # å°†ç³»ç»Ÿæç¤ºè¯æ·»åŠ åˆ°ç”¨æˆ·æ¶ˆæ¯çš„å¼€å¤´
                    original_content = msg["content"]
                    messages[i]["content"] = f"{system_prompt}\n\n{original_content}"
                    break
            else:
                # å¦‚æœæ²¡æœ‰ç”¨æˆ·æ¶ˆæ¯ï¼Œåˆ›å»ºä¸€ä¸ªåŒ…å«ç³»ç»Ÿæç¤ºè¯çš„ç”¨æˆ·æ¶ˆæ¯
                messages.append({"role": "user", "content": system_prompt})
        
        # æ„å»ºè¯·æ±‚å‚æ•°
        params = {
            "model": model_name,
            "messages": messages,
        }
        
        if temperature is not None:
            params["temperature"] = temperature
        if top_p is not None:
            params["top_p"] = top_p
        if max_tokens is not None:
            params["max_tokens"] = max_tokens
        
        # æ·»åŠ JSONæ ¼å¼åŒ–è¾“å‡ºæ”¯æŒ (LM Studio)
        if response_format is not None:
            params["response_format"] = response_format
            print(f"ğŸ”§ LM Studioä½¿ç”¨ç»“æ„åŒ–è¾“å‡º: {response_format.get('type', 'unknown')}")
        
        # æ·»åŠ tool callingæ”¯æŒ (LM Studio)
        if tools is not None:
            params["tools"] = tools
            if tool_choice is not None:
                params["tool_choice"] = tool_choice
            print(f"ğŸ”§ LM Studioä½¿ç”¨å·¥å…·è°ƒç”¨: {len(tools)}ä¸ªå·¥å…·")
        
        try:
            if not stream:
                response = client.chat.completions.create(**params)
                return {
                    "content": response.choices[0].message.content,
                    "total_tokens": response.usage.total_tokens if response.usage else 0,
                }
            else:
                # å¯¹äºæµå¼è¾“å‡ºï¼Œä¸æ”¯æŒç»“æ„åŒ–è¾“å‡º
                stream_params = params.copy()
                stream_params["stream"] = True
                if "response_format" in stream_params:
                    del stream_params["response_format"]
                    print("âš ï¸  LM Studioæµå¼è¾“å‡ºä¸æ”¯æŒç»“æ„åŒ–è¾“å‡ºï¼Œå·²è‡ªåŠ¨ç§»é™¤")
                
                responses = client.chat.completions.create(**stream_params)

                def respGenerator():
                    content = ""
                    for response in responses:
                        delta = response.choices[0].delta.content
                        if delta:
                            content += delta

                        # LM Studio å¯èƒ½ä¸æä¾› usage ä¿¡æ¯
                        total_tokens = 0
                        if hasattr(response, 'usage') and response.usage:
                            total_tokens = response.usage.total_tokens

                        yield {
                            "content": content,
                            "total_tokens": total_tokens,
                        }

                return respGenerator()
                
        except Exception as e:
            print(f"âŒ LM Studio API è°ƒç”¨å¤±è´¥: {e}")
            print(f"è¯·ç¡®ä¿ LM Studio æ­£åœ¨è¿è¡Œå¹¶ç›‘å¬ {base_url}")
            raise e

    return chatLLM