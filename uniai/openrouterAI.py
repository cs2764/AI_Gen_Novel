import os
from openai import OpenAI

def openrouterChatLLM(model_name="openai/gpt-4", api_key=None, system_prompt="", base_url=None, provider_routing=None):
    """
    OpenRouter AI Chat LLM with Provider Routing Support
    
    Args:
        model_name: æ¨¡å‹åç§°ï¼Œä¾‹å¦‚ openai/gpt-4, deepseek/deepseek-chat ç­‰
        api_key: OpenRouter APIå¯†é’¥
        system_prompt: ç³»ç»Ÿæç¤ºè¯
        base_url: APIåŸºç¡€URL
        provider_routing: æä¾›å•†è·¯ç”±é…ç½®ï¼ŒåŒ…å« order, allow_fallbacks, sort ç­‰å‚æ•°
    
    model_name å–å€¼ç¤ºä¾‹:
    - openai/gpt-4
    - openai/gpt-3.5-turbo
    - anthropic/claude-3-opus
    - anthropic/claude-3-sonnet
    - anthropic/claude-3-haiku
    - google/gemini-pro
    - meta-llama/llama-2-70b-chat
    - mistralai/mixtral-8x7b-instruct
    - deepseek/deepseek-chat
    - deepseek/deepseek-coder
    """
    api_key = os.environ.get("OPENROUTER_API_KEY", api_key)
    
    # ä½¿ç”¨ä¼ å…¥çš„base_urlæˆ–é»˜è®¤å€¼
    actual_base_url = base_url or "https://openrouter.ai/api/v1"
    
    # ä½¿ç”¨OpenRouterçš„APIç«¯ç‚¹
    client = OpenAI(
        api_key=api_key,
        base_url=actual_base_url,
        timeout=1200.0,  # 20åˆ†é’Ÿè¶…æ—¶
        default_headers={
            "HTTP-Referer": "https://github.com/cjyyx/AI_Gen_Novel",  # å¯é€‰ï¼Œç”¨äºè·Ÿè¸ª
            "X-Title": "AI Novel Generator",  # å¯é€‰ï¼Œåº”ç”¨åç§°
        }
    )

    def chatLLM(
        messages: list,
        temperature=None,
        top_p=None,
        max_tokens=None,
        stream=False,
        response_format=None,
        tools=None,
        tool_choice=None,
    ) -> dict:
        
        # å¦‚æœè®¾ç½®äº†ç³»ç»Ÿæç¤ºè¯ï¼Œåˆå¹¶åˆ°ç¬¬ä¸€ä¸ªç”¨æˆ·æ¶ˆæ¯çš„å¼€å¤´
        if system_prompt and messages:
            # æ‰¾åˆ°ç¬¬ä¸€ä¸ªç”¨æˆ·æ¶ˆæ¯
            for i, msg in enumerate(messages):
                if msg.get("role") == "user":
                    # å°†ç³»ç»Ÿæç¤ºè¯æ·»åŠ åˆ°ç”¨æˆ·æ¶ˆæ¯çš„å¼€å¤´
                    original_content = msg["content"]
                    messages[i]["content"] = f"{system_prompt}\n\n{original_content}"
                    break
            else:
                # å¦‚æœæ²¡æœ‰ç”¨æˆ·æ¶ˆæ¯ï¼Œåˆ›å»ºä¸€ä¸ªåŒ…å«ç³»ç»Ÿæç¤ºè¯çš„ç”¨æˆ·æ¶ˆæ¯
                messages.append({"role": "user", "content": system_prompt})
        
        # æ„å»ºè¯·æ±‚å‚æ•°
        params = {
            "model": model_name,
            "messages": messages,
        }
        
        if temperature is not None:
            params["temperature"] = temperature
        if top_p is not None:
            params["top_p"] = top_p
        if max_tokens is not None:
            params["max_tokens"] = max_tokens
        
        # æ·»åŠ structured outputsæ”¯æŒ (OpenRouter)
        if response_format is not None:
            params["response_format"] = response_format
            print(f"ğŸ”§ OpenRouterä½¿ç”¨ç»“æ„åŒ–è¾“å‡º: {response_format.get('type', 'unknown')}")
        
        # æ·»åŠ tool callingæ”¯æŒ (OpenRouter)
        if tools is not None:
            params["tools"] = tools
            if tool_choice is not None:
                params["tool_choice"] = tool_choice
            print(f"ğŸ”§ OpenRouterä½¿ç”¨å·¥å…·è°ƒç”¨: {len(tools)}ä¸ªå·¥å…·")
        
        # æ·»åŠ æä¾›å•†è·¯ç”±é…ç½®
        if provider_routing:
            print(f"ğŸ”€ OpenRouterä½¿ç”¨æä¾›å•†è·¯ç”±é…ç½®: {provider_routing}")
            # æ ¹æ®OpenRouteræ–‡æ¡£ï¼Œprovideréœ€è¦é€šè¿‡extra_bodyå‚æ•°ä¼ é€’
            if "extra_body" not in params:
                params["extra_body"] = {}
            params["extra_body"]["provider"] = provider_routing
        
        try:
            if not stream:
                response = client.chat.completions.create(**params)
                
                # å¤„ç†tool callingå“åº”
                if tools and response.choices[0].message.tool_calls:
                    return {
                        "content": response.choices[0].message.content,
                        "tool_calls": response.choices[0].message.tool_calls,
                        "total_tokens": response.usage.total_tokens if response.usage else 0,
                    }
                else:
                    return {
                        "content": response.choices[0].message.content,
                        "total_tokens": response.usage.total_tokens if response.usage else 0,
                    }
            else:
                params["stream"] = True
                responses = client.chat.completions.create(**params)

                def respGenerator():
                    content = ""
                    total_tokens = 0
                    
                    for response in responses:
                        if response.choices and response.choices[0].delta.content:
                            delta = response.choices[0].delta.content
                            content += delta
                            
                            # OpenRouterå¯èƒ½ä¸æä¾›æµå¼çš„tokenç»Ÿè®¡ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¼°ç®—
                            total_tokens = len(content.split()) * 1.3
                            
                            yield {
                                "content": content,
                                "total_tokens": int(total_tokens),
                            }

                return respGenerator()
                
        except Exception as e:
            raise ValueError(f"OpenRouter APIè°ƒç”¨å¤±è´¥: {str(e)}")

    return chatLLM