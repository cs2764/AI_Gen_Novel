import os
from openai import OpenAI


def lambdaChatLLM(model_name="llama-4-maverick-17b-128e-instruct-fp8", api_key=None, system_prompt="", base_url=None):
    """
    Lambda AI Chat LLM using OpenAI-compatible API
    
    Args:
        model_name: Lambda model name (default: llama-4-maverick-17b-128e-instruct-fp8)
        api_key: Lambda API key
        system_prompt: System prompt to prepend to user messages
        base_url: API base URL (default: https://api.lambda.ai/v1)
    
    model_name å–å€¼ç¤ºä¾‹:
    - llama-4-maverick-17b-128e-instruct-fp8
    - llama-4-scout-17b-16e-instruct
    - deepseek-r1-0528
    - deepseek-v3-0324
    - llama3.1-8b-instruct
    - llama3.1-70b-instruct-fp8
    - llama3.1-405b-instruct-fp8
    - llama3.3-70b-instruct-fp8
    - qwen3-32b-fp8
    - hermes3-8b
    - hermes3-70b
    - hermes3-405b
    """
    api_key = os.environ.get("LAMBDA_API_KEY", api_key)
    
    # ä½¿ç”¨ä¼ å…¥çš„base_urlæˆ–é»˜è®¤å€¼
    actual_base_url = base_url or "https://api.lambda.ai/v1"
    
    # ä½¿ç”¨Lambdaçš„APIç«¯ç‚¹
    client = OpenAI(
        api_key=api_key,
        base_url=actual_base_url,
        timeout=1800.0,  # 30åˆ†é’Ÿè¶…æ—¶
    )

    def chatLLM(
        messages: list,
        temperature=None,
        top_p=None,
        max_tokens=None,
        stream=False,
    ) -> dict:

        
        # Lambda AIé»˜è®¤max_tokensè®¾ç½®ä¸º20000ï¼ˆç¡®ä¿ç« èŠ‚å†…å®¹ä¸è¢«æˆªæ–­ï¼‰
        if max_tokens is None:
            max_tokens = 20000
        
        # å¦‚æœè®¾ç½®äº†ç³»ç»Ÿæç¤ºè¯ï¼Œåˆå¹¶åˆ°ç¬¬ä¸€ä¸ªç”¨æˆ·æ¶ˆæ¯çš„å¼€å¤´
        if system_prompt and messages:
            # æ‰¾åˆ°ç¬¬ä¸€ä¸ªç”¨æˆ·æ¶ˆæ¯
            for i, msg in enumerate(messages):
                if msg.get("role") == "user":
                    # å°†ç³»ç»Ÿæç¤ºè¯æ·»åŠ åˆ°ç”¨æˆ·æ¶ˆæ¯çš„å¼€å¤´
                    original_content = msg["content"]
                    messages[i]["content"] = f"{system_prompt}\n\n{original_content}"
                    break
            else:
                # å¦‚æœæ²¡æœ‰ç”¨æˆ·æ¶ˆæ¯ï¼Œåˆ›å»ºä¸€ä¸ªåŒ…å«ç³»ç»Ÿæç¤ºè¯çš„ç”¨æˆ·æ¶ˆæ¯
                messages.append({"role": "user", "content": system_prompt})
        
        # æ„å»ºè¯·æ±‚å‚æ•°
        params = {
            "model": model_name,
            "messages": messages,
        }
        
        # ZenMux APIæ”¯æŒtemperatureå‚æ•°,ä½†éœ€è¦ç¡®ä¿åœ¨æœ‰æ•ˆèŒƒå›´å†…
        # æ ¹æ®æ–‡æ¡£,é€šå¸¸èŒƒå›´æ˜¯0-2,ä½†æŸäº›æ¨¡å‹(å¦‚Claude)èŒƒå›´æ˜¯0-1
        if temperature is not None:
            try:
                # ç¡®ä¿temperatureæ˜¯æ•°å­—ç±»å‹
                temp_value = float(temperature)
                # ç¡®ä¿åœ¨åˆç†èŒƒå›´å†…,é¿å…APIé”™è¯¯
                validated_temp = max(0.0, min(2.0, temp_value))
                if validated_temp != temp_value:
                    print(f"âš ï¸ Temperature {temp_value} è¶…å‡ºèŒƒå›´,å·²è°ƒæ•´ä¸º {validated_temp}")
                params["temperature"] = validated_temp
                print(f"ğŸ”§ Lambda API: è®¾ç½® temperature = {validated_temp} (åŸå§‹å€¼: {temperature}, ç±»å‹: {type(temperature)})")
            except (TypeError, ValueError) as e:
                print(f"âŒ Temperature å‚æ•°æ— æ•ˆ: {temperature} (ç±»å‹: {type(temperature)}), é”™è¯¯: {e}")
                print(f"âš ï¸ è·³è¿‡ temperature å‚æ•°,ä½¿ç”¨APIé»˜è®¤å€¼")
        if top_p is not None:
            params["top_p"] = top_p
        if max_tokens is not None:
            params["max_tokens"] = max_tokens  # ä¿ç•™åŸå§‹å‚æ•°
            params["max_completion_tokens"] = max_tokens  # æ·»åŠ zenmux APIå‚æ•°ï¼Œé™åˆ¶æ¨¡å‹ç”Ÿæˆå†…å®¹é•¿åº¦ï¼ˆåŒ…æ‹¬æ¨ç†è¿‡ç¨‹ï¼‰
        
        try:
            if not stream:
                response = client.chat.completions.create(**params)
                return {
                    "content": response.choices[0].message.content,
                    "total_tokens": response.usage.total_tokens if response.usage else 0,
                }
            else:
                params["stream"] = True
                responses = client.chat.completions.create(**params)

                def respGenerator():
                    content = ""
                    total_tokens = 0
                    
                    for response in responses:
                        if response.choices and response.choices[0].delta.content:
                            delta = response.choices[0].delta.content
                            content += delta
                            
                            # ä¼°ç®—tokenæ•°é‡
                            total_tokens = len(content.split()) * 1.3
                            
                            yield {
                                "content": content,
                                "total_tokens": int(total_tokens),
                            }

                return respGenerator()
                
        except Exception as e:
            raise ValueError(f"Lambda APIè°ƒç”¨å¤±è´¥: {str(e)}")

    return chatLLM 